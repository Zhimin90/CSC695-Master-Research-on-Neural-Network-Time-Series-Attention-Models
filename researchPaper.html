<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=yrPxF2fzIbKO47OcVFfZ2CvhBULzVSj67-CyYQmQBzclLT34rKZNjX_x5wwcqB1FJcp4QFgPPqKvq4Bp_6u4rQ);.lst-kix_ewqq6femr54-0>li{counter-increment:lst-ctn-kix_ewqq6femr54-0}.lst-kix_ewqq6femr54-6>li{counter-increment:lst-ctn-kix_ewqq6femr54-6}ol.lst-kix_ewqq6femr54-3.start{counter-reset:lst-ctn-kix_ewqq6femr54-3 0}.lst-kix_ewqq6femr54-3>li{counter-increment:lst-ctn-kix_ewqq6femr54-3}ol.lst-kix_ewqq6femr54-7.start{counter-reset:lst-ctn-kix_ewqq6femr54-7 0}ol.lst-kix_ewqq6femr54-2.start{counter-reset:lst-ctn-kix_ewqq6femr54-2 0}ol.lst-kix_ewqq6femr54-8.start{counter-reset:lst-ctn-kix_ewqq6femr54-8 0}.lst-kix_ewqq6femr54-1>li{counter-increment:lst-ctn-kix_ewqq6femr54-1}.lst-kix_ewqq6femr54-4>li{counter-increment:lst-ctn-kix_ewqq6femr54-4}.lst-kix_ewqq6femr54-7>li{counter-increment:lst-ctn-kix_ewqq6femr54-7}ol.lst-kix_ewqq6femr54-0.start{counter-reset:lst-ctn-kix_ewqq6femr54-0 0}ol.lst-kix_ewqq6femr54-5.start{counter-reset:lst-ctn-kix_ewqq6femr54-5 0}ol.lst-kix_ewqq6femr54-4.start{counter-reset:lst-ctn-kix_ewqq6femr54-4 0}.lst-kix_ewqq6femr54-6>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-6,decimal) ". "}.lst-kix_ewqq6femr54-7>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-7,lower-latin) ". "}.lst-kix_ewqq6femr54-4>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-4,lower-latin) ". "}.lst-kix_ewqq6femr54-5>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-5,lower-roman) ". "}.lst-kix_ewqq6femr54-8>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-8,lower-roman) ". "}ol.lst-kix_ewqq6femr54-1.start{counter-reset:lst-ctn-kix_ewqq6femr54-1 0}.lst-kix_ewqq6femr54-8>li{counter-increment:lst-ctn-kix_ewqq6femr54-8}.lst-kix_ewqq6femr54-5>li{counter-increment:lst-ctn-kix_ewqq6femr54-5}ol.lst-kix_ewqq6femr54-8{list-style-type:none}ol.lst-kix_ewqq6femr54-7{list-style-type:none}.lst-kix_ewqq6femr54-0>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-0,upper-latin) ". "}.lst-kix_ewqq6femr54-1>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-1,lower-latin) ". "}.lst-kix_ewqq6femr54-2>li{counter-increment:lst-ctn-kix_ewqq6femr54-2}ol.lst-kix_ewqq6femr54-6{list-style-type:none}ol.lst-kix_ewqq6femr54-5{list-style-type:none}ol.lst-kix_ewqq6femr54-6.start{counter-reset:lst-ctn-kix_ewqq6femr54-6 0}ol.lst-kix_ewqq6femr54-4{list-style-type:none}ol.lst-kix_ewqq6femr54-3{list-style-type:none}.lst-kix_ewqq6femr54-2>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-2,lower-roman) ". "}ol.lst-kix_ewqq6femr54-2{list-style-type:none}.lst-kix_ewqq6femr54-3>li:before{content:"" counter(lst-ctn-kix_ewqq6femr54-3,decimal) ". "}ol.lst-kix_ewqq6femr54-1{list-style-type:none}ol.lst-kix_ewqq6femr54-0{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c4{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c55{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:183pt;border-top-color:#000000;border-bottom-style:solid}.c42{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:189pt;border-top-color:#cccccc;border-bottom-style:solid}.c43{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.2pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c49{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c38{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#ffffff;border-top-width:1pt;border-right-width:1pt;border-left-color:#ffffff;vertical-align:top;border-right-color:#ffffff;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:174.8pt;border-top-color:#ffffff;border-bottom-style:solid}.c13{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:93.8pt;border-top-color:#000000;border-bottom-style:solid}.c47{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#ffffff;border-top-width:1pt;border-right-width:1pt;border-left-color:#ffffff;vertical-align:top;border-right-color:#ffffff;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:293.2pt;border-top-color:#ffffff;border-bottom-style:solid}.c8{margin-left:36pt;padding-top:10pt;padding-left:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c16{background-color:#ffffff;color:#222222;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c21{margin-left:-0.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:11pt}.c19{margin-left:-0.8pt;padding-top:24pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left;margin-right:89.2pt}.c29{margin-left:36pt;padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c59{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c39{color:#999999;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Economica";font-style:normal}.c45{margin-left:-0.8pt;padding-top:3pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c52{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:30pt;font-family:"Economica";font-style:normal}.c14{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c17{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c27{margin-left:-0.8pt;padding-top:6pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c9{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Open Sans";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Open Sans";font-style:italic}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c60{margin-left:-0.8pt;padding-top:30pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Open Sans";font-style:normal}.c44{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Economica";font-style:normal}.c28{margin-left:-0.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c58{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c34{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Open Sans";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans"}.c31{color:#000000;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Open Sans"}.c20{background-color:#ffffff;font-size:10pt;font-family:"Arial";color:#222222;font-weight:400}.c40{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c51{margin-left:-0.8pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c36{border-spacing:0;border-collapse:collapse;margin-right:auto}.c61{padding-top:0pt;padding-bottom:0pt;line-height:2.0;text-align:left}.c10{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c18{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{font-weight:400;font-family:"Courier New"}.c0{vertical-align:sub;font-style:italic}.c57{color:#000000;font-size:30pt}.c41{padding:0;margin:0}.c56{font-size:11pt;font-weight:400}.c25{color:inherit;text-decoration:inherit}.c50{vertical-align:super}.c33{height:11pt}.c26{height:16pt}.c46{height:26.2pt}.c62{background-color:#ff0000}.c24{vertical-align:sub}.c54{font-size:16pt}.c53{font-weight:700}.c35{font-size:10pt}.c6{font-style:italic}.c23{text-indent:0.8pt}.c15{height:16.5pt}.c30{height:90pt}.c48{height:15.8pt}.title{padding-top:0pt;color:#000000;font-size:30pt;padding-bottom:0pt;font-family:"Economica";line-height:1.0;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#999999;font-size:14pt;padding-bottom:0pt;font-family:"Economica";line-height:1.0;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Open Sans"}p{margin:0;color:#000000;font-size:11pt;font-family:"Open Sans"}h1{padding-top:10pt;color:#000000;font-weight:700;font-size:16pt;padding-bottom:0pt;font-family:"Open Sans";line-height:1.5;orphans:2;widows:2;text-align:left}h2{padding-top:24pt;color:#000000;font-weight:700;font-size:13pt;padding-bottom:0pt;font-family:"Open Sans";line-height:1.0;orphans:2;widows:2;text-align:left}h3{padding-top:10pt;color:#8c7252;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Open Sans";line-height:1.5;orphans:2;widows:2;text-align:left}h4{padding-top:8pt;-webkit-text-decoration-skip:none;color:#666666;text-decoration:underline;font-size:11pt;padding-bottom:0pt;line-height:1.5;page-break-after:avoid;text-decoration-skip-ink:none;font-family:"Trebuchet MS";orphans:2;widows:2;text-align:left}h5{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.5;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c18"><div><p class="c33 c60 subtitle" id="h.leajue2ys1lr"><span class="c39"></span></p><p class="c28"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 2.67px;"><img alt="" src="images/image37.png" style="width: 624.00px; height: 2.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="horizontal line"></span></p></div><p class="c28"><span class="c44">CSC 695 Masters Research</span></p><p class="c34 c23 title" id="h.mbjsiz6n6jlo"><span class="c53">An Investigation in Chicago City Crime Prediction with RNN plus Attention Models</span></p><p class="c28 subtitle" id="h.vb8p0lepu9vn"><span class="c57">Zhimin Zou</span></p><p class="c45"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 4.00px;"><img alt="" src="images/image51.png" style="width: 624.00px; height: 4.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="horizontal line"></span></p><h1 class="c27" id="h.vydniszftb1n"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 505.46px; height: 587.50px;"><img alt="" src="images/image14.png" style="width: 505.46px; height: 587.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c14 c33"><span class="c1"></span></p><h1 class="c14" id="h.arolcxe0i15c"><span>Abstract</span></h1><p class="c14 c23"><span>With techniques derived from the state-of-the-art Natural Language Processing models, a novel Fast Fourier Transformation Attention model has been crafted to predict the Kernel Density Estimate, KDE, of Chicago Crime Data. Attention binded with LSTM Encoder Decoder has outperformed LSTM Encoder Decoder in predictive power for aggregated Chicago crime occurrences 4 week forecast. With frequency domain based context, Attention model has been found to be more transparent and insightful than non-attention models while providing greater performance than </span><span>its</span><span>&nbsp;none-Attention model counterpart.</span></p><h1 class="c14" id="h.wqicl2cyp17s"><span class="c9">Introduction</span></h1><p class="c14 c23"><span>Crime is one of the most notable social issues in Chicago. Being able to visualize the near future occurrence of crime can be a valuable tool for Chicago Police Department, Hospitals, and Chicago residents. The goal of this research is to create a model that provides the most accurate forecast of crime in the form of Kernel Density Estimate, KDE, of crime occurrences in spatial and time dimensions. With a forecast KDE visualized as a heat map of near future occurrence of crime, the city of Chicago will be able to allocate solutions to prevent crimes.</span></p><h1 class="c59" id="h.ckrx6bmzzp2s"><span class="c9">Data Summary</span></h1><h2 class="c19" id="h.ix8y7o15b0hw"><span class="c11">Data Source</span></h2><p class="c14 c23"><span>The Chicago Data Portal has detailed records of crimes by different categories but most importantly time and location of the crime from 2001 to today </span><span class="c10"><a class="c25" href="#h.eensspbhgc0a">[1]</a></span><span class="c1">. Each row of this dataset represents a single occurrence of a crime.</span></p><h2 class="c19" id="h.kew4pib3wki"><span class="c11">Data Transformation</span></h2><p class="c14 c23"><span class="c1">To achieve a macroscale visualization on the scale of a whole city, individual instances of crime location are aggregated into a matrix of 25 pixels by 25 pixels for a given time interval range of 30 days. The method used for this data transformation is the gaussian Kernel Density Estimate, KDE. Each pixel is assigned a single value that represents the density of the crime. A time dimension is created by sliding a 30 days window with increments of 7 days. 52 slices of 25 pixels by &nbsp;25 pixels matrix is created as a single sample input into the models with the goal of predicting 4 slices of 30 days crime aggregate information of 25 pixels by 25 pixels into the future. Each of these 4 slices are shifted with length 7 days into the future. Approximately 1000 samples were generated with crime data from January 2001 to June 2020. These 1000 samples are then split between the training set, validation set, and test set. The data input format is a sequence of 52 and the output data format is a sequence of 4. This makes the model presented to be sequence to sequence models. With this format, parallels can be drawn from inspirational models from the Natural Language Processing domain. Since time is a continuous sequence, this data format can also be converted to many to one format. Each time step can be generated autoregressive by using previously predicted data as input for the next time step. The choice of choosing sequence to sequence format is based on the ease of comparison for Attention at a varying range of time of prediction.</span></p><h2 class="c19" id="h.h3j5x3uze12y"><span class="c11">Data Exploration</span></h2><p class="c14"><span>As part of a data exploration and visualization, a D3.js driven dashboard built specifically for Chicago Crime Data has been used for data exploration </span><span class="c10"><a class="c25" href="#h.eensspbhgc0a">[2]</a></span><span class="c1">. This dashboard allows the crime to be visualized with context of the street name and wards by leveraging Mapbox GL technology. Crime heatmap is projected onto Mapbox in a sliding window of monthly range and monthly interval. Crime data can be grouped by wards, crime type, and location type. Crime data can be visualized in time series aggregate in a 24 hours period or by the whole current year. Seasonality is observed in a 24 hours cycle and yearly cycle during the data exploration phase of this project. Trend is observed in the monthly time range with the dashboard explorer.</span></p><p class="c14"><span>As part of the model troubleshooting process, autocorrelation and partial autocorrelation are charted for KDE pixels with the greatest density. Sample of these charts are in the Appendix A section. Autocorrelation and Partial autocorrelation charts revealed the relationship between the most dense crime blocks has high autocorrelation with lag 1 time step or 7 days intervals and also lag 4 to 6 time steps, which are 28 to 48 day lags. Autocorrelation gave insight to the understanding of the crime without relation to neighboring pixels or locations. Traditional ARIMA models will be well suited if there is no correlation between crime of nearby locations. However, by intuition and data visualization, there will be correlations between features or pixels in our data. For this reason, various neural network models will be used to capture these cross-correlation location dependencies.</span></p><h1 class="c14" id="h.qrdmcgs6r5wi"><span class="c9">Methods</span></h1><p class="c14"><span class="c1">Various Neural Network models are explored to test the capability of the predictive power. The loss function used for these models is Mean Absolute Error, MAE. With the inputs to the model formatted as 52 slices of KDE and output of the model formatted as 4 slices of KDE, the network is defined as a sequence to sequence model.</span></p><p class="c14"><span class="c1">The data set is split into 60 percent of the samples for training, 20 percent for validation, and 20 percent for testing. The training set is used to generate the min and max scale for min-max scaler to achieve the value range of 0 to 1.0. This scaler is then applied to the validation and test set.</span></p><h2 class="c19" id="h.9hkciqwy2to2"><span class="c6 c56">Figure 1a Encoder Decoder Model</span></h2><h2 class="c19" id="h.hi6q1r1ycias"><span>Encoder Decoder</span><span class="c11">&nbsp;Model</span></h2><p class="c14"><span>The encoder decoder model is the most simple Neural Network sequence to sequence model. The typical application for this model is Natural Language Processing, specifically translation. An encoder decoder model consists of two sections. The first section is a single layer Encoder LSTM with 100 units. After stepping through 52 slices of the flattened 25 by 25 pixels as time step, the latest hidden state of the Encoder LSTM out is copied 4 times. This creates 4 slices or time steps which are then inputted into the Decoder LSTM layer. The decoder LSTM layer is set to return the full sequence of its hidden states. This allows the output to create a tensor with dimensions as &nbsp;(sample_size, time step, features). See </span><span class="c6">Figure 1a</span><span class="c1">&nbsp;for model architecture diagram.</span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 125.00px; height: 362.50px;"><img alt="" src="images/image15.png" style="width: 125.00px; height: 362.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">The 2 layer LSTM Encoder Decoder model performed extremely well in capturing the relationship between the time steps and the relationships between pixels. The validation set MAE is 0.00501 and test set MAE is 0.008138. The average error is less than 1% of the respective pixel value which can range from approximately 0 to 1.0 after being scaled. Notably, there has been a sharp change in crime patterns in the test set data due to COVID-19 being in the test set data as the test set is the last 200 samples of the time series.</span></p><p class="c14"><span>The output of this model in validation set is charted as pixel wise time series to examine the trend, cycle, and seasonal captured by the model. The prediction for each sample given the pixel number is captured with the same absolute time for a given time enumeration on x-axis. This is achieved by padding the front of the predicted pixel wise time series with empty values to align them to reference the same week for a given x-axis value. These charts can be found in Appendix B.</span></p><h2 class="c19" id="h.yk8cdf3eevdl"><span class="c11">Attention Models</span></h2><p class="c14"><span>In the recent conception of the Transformer model in Natural Language Processing, Attention has been researched and applied in a variety of different fields such as translation, sentence generation, image generation and time series </span><span class="c10"><a class="c25" href="#h.eensspbhgc0a">[3]</a></span><span class="c1">. Attention allows the network to skip superfluous information being propagated in the training process. The big concept of Attention is assigning weights to features that are more correlated to efficiently generate the correct prediction under given context of the prediction.</span></p><p class="c14"><span>While there are many parallels in processing time series data versus processing sentences, NLP style Attention cannot be directly applied to time series data. There are limitations to traditional attention from natural language processing in time series. Typically, the weight vector of Attention is divided between timesteps or in NLP words. The results of the output for NLP styled Attention is generating a sentence where each time step is a single word embedding. Specifically, in </span><span class="c6">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE </span><span class="c10"><a class="c25" href="#h.eensspbhgc0a">[4]</a></span><span class="c6">,</span><span>&nbsp;context vector </span><img src="images/image1.png"><span>is defined for each word, i.</span><span>&nbsp;The attention weight </span><img src="images/image2.png"><span>is defined in word i and annotations &nbsp;j dimensions. In order to generate context, the annotation j dimension is aggregated together as sum. This collapses sequential information of previous RNN hidden state </span><img src="images/image3.png"><span>. Each attention is based on a single time step in RNN hidden state. Using this style of Attention multivariate time series is limited. A correlation of many time steps to one time step can certainly be derived in seasonal forecasting. </span><span>NLP</span><span>&nbsp;style Attention mechanism averages information over multiple time steps, therefore attending to time steps is not the optimal solution in multivariate time series </span><span class="c10"><a class="c25" href="#h.eensspbhgc0a">[5]</a></span><span class="c1">. </span></p><p class="c14"><span>Inspired by the 1D-CNN with Attention to features model presented in </span><span class="c6">Temporal Pattern Attention for Multivariate Time Series Forecasting</span><span>,</span><span>&nbsp;leveraging Attention mechanisms to capture important information for given context. A novel Fast Fourier Transformation, FFT, Attention has been found to be the most optimal in time series forecasting crime data. In order to circumvent the limitation of selecting a single relevant time step for attention weight, </span><span class="c6">Temporal Pattern Attention for Multivariate Time Series Forecasting </span><span>presented an Attention model that selects the most relevant time series, which has multiple timesteps information factored into context vector </span><img src="images/image1.png"><span>. T</span><span class="c6">emporal Pattern Attention for Multivariate Time Series Forecasting</span><span>&nbsp;highlighted the limitation of using a single time step since in order to predict time series patterns, changes in multiple time steps have been factored into a single time step forecast. </span></p><p class="c14"><span class="c1">An alternative frequency dimension is generated to circumvent this limitation. In order to capture the many time steps to one dependency in Attention mechanism, a summary of the time range&rsquo;s seasonal and trend information is captured into a vector representation. FFT is used to achieve this summary of time range seasonal and trend information which contains multiple timesteps information for a given frequency. FFT is chosen particularly, for it is a computationally efficient implementation of Discrete Fourier Transformation, DFT that is widely available as a standard library.</span></p><p class="c14"><span>At a high level, DFT measures the correlation or similarity between the input time domain data with a discrete set of varying period sinusoidal functions. These different frequency domain waves are effectively possible seasons in the input data with incremental variation in periods. In the equation below, </span><img src="images/image4.png"><span>&nbsp;represents the original discrete time series signal where </span><img src="images/image4.png"><span>&nbsp;is the value of crime density from 1 to 52 weeks. N is the top range of frequency bins. N is limited to less than 52 weeks for DFT. For TensorFlow implementation of Real FFT, N is set to be Nyquist rate, minimum rate which finite bandwidth needs to be sampled to retain complete information, of 0.5 of sampling time interval of 52 weeks plus 0 frequency bin which is 27 frequency bins. Effectively, the 0 frequency bin of the discrete series of DFT is the mean. The term </span><img src="images/image5.png"><span>is a standard sinusoidal wave at the given frequency bin n decomposed in discrete form to match </span><img src="images/image4.png"><span>. The following DFT equation below measures shows that a similarity measure is taken per frequency bin for the term </span><img src="images/image4.png"><span>&nbsp;and </span><img src="images/image5.png"><span class="c1">. Figure 4 below shows two encoder LSTM cell outputs that demonstrate the time domain versus frequency domain.</span></p><p class="c17"><span>DFT: </span><img src="images/image6.png"><span>&nbsp;where k = 0,1,...N-1 and </span><img src="images/image7.png"></p><p class="c14"><span>With the 2 layer encoder decoder LSTM model as foundation, Attention mechanism is applied to the hidden states of the encoder. See </span><span class="c6">Figure 1b</span><span class="c1">&nbsp;for model architecture diagram. With the transformed 100 unit encoder LSTM hidden states from time steps of 52 into frequency domain of 27, the output of the FFT is a tensor with shape of 27 frequency by 100 LSTM unit where each of the 27 frequency is a summary of the seasonality at an incremental increase in period by 7 days with 7 days as the highest frequency and 196 days as the lowest frequency. Visualization of 52 days of crime KDE at various frequencies is in Appendix E. </span></p><p class="c14 c33"><span class="c1"></span></p><a id="t.74638c81bb135697544d1c90f82f246cf40f00b2"></a><a id="t.0"></a><table class="c51"><tbody><tr class="c30"><td class="c47" colspan="1" rowspan="1"><p class="c34"><span class="c6">H</span><span class="c0">f</span><span class="c6">: </span><span class="c2">Frequency domain LSTM hidden states vector</span></p><p class="c34 c33"><span class="c12 c6"></span></p><p class="c34"><span class="c6">W</span><span class="c0">a</span><span class="c6">: </span><span class="c2">Dense Layer weights for projection</span></p><p class="c34 c33"><span class="c6 c12"></span></p><p class="c34"><span class="c6">h</span><span class="c0">t</span><span class="c6">: </span><span class="c2">Latest hidden state from LSTM</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c34"><span class="c6">f(H</span><span class="c0">f</span><span class="c6">,h</span><span class="c0">t</span><span class="c6">) = H</span><span class="c0">f</span><span class="c6 c50">T</span><span class="c6">&nbsp;W</span><span class="c0">a</span><span class="c6">h</span><span class="c0 c31">t</span></p><p class="c34 c33"><span class="c12 c6"></span></p><p class="c34"><span class="c6">&alpha;</span><span class="c0">f </span><span class="c6">= Sigmoid(f(H</span><span class="c0">f</span><span class="c6">,h</span><span class="c0">t</span><span class="c12 c6">))</span></p><p class="c34 c33"><span class="c12 c6"></span></p><p class="c34"><span class="c6">Attention</span><span class="c0">out</span><span class="c6">&nbsp;= &alpha;</span><span class="c0">f</span><span class="c50 c6">T </span><span class="c6">H</span><span class="c0">f</span></p></td></tr></tbody></table><p class="c14"><span>In order to attend to LSTM out in frequency domain H</span><span class="c24">f</span><span>, a similarity measure between the latest encoder LSTM hidden state h</span><span class="c24">t</span><span>&nbsp;and H</span><span class="c24">f</span><span>. To keep the output dimension in frequency domain </span><img src="images/image8.png"><span>, a dense layer W</span><span class="c24">a</span><span>&nbsp;is applied to the latest hidden layer in the time domain. The inner product of H</span><span class="c24">f</span><span>&nbsp;and W</span><span class="c24">a</span><span>h</span><span class="c24">t</span><span>&nbsp;is then applied to with the sigmoid activation function to create a similarity score per frequency to produce a normalized attention weight </span><img src="images/image9.png"><span>. Each LSTM unit frequency domain vector is then summarized in frequency with the inner product of </span><img src="images/image9.png"><span>&nbsp;and H</span><span class="c24">f</span><span class="c1">&nbsp;giving each LSTM cell unit aggregate value based on the most relevant frequencies summed together. This allows pixel wise information to be represented across multiple time steps.</span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 612.54px; height: 793.17px;"><img alt="" src="images/image11.png" style="width: 612.54px; height: 793.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17"><span class="c6">Figure 1b: LSTM Attention to Frequency Domain Hidden States</span></p><p class="c14"><span>Both hidden states in time dimension h</span><span class="c24">t</span><span>&nbsp;and weighted hidden state Attention</span><span class="c24">out</span><span class="c1">&nbsp;from Attention are then applied with a dense layer and then concatenated to form the hidden representation for the decoder LSTM with 100 units. A final dense layer is then applied on the output of the LSTM unit for transforming the output into the original input feature shape of 625 pixels or 25 by 25 pixels.</span></p><p class="c14"><span>With the encoder decoder architecture, each output time step can be attended independently with different sets of weights. This allows each of the decoder LSTM units to focus on different parts of the encoder LSTM outputs. For output with 4 time steps, 4 different attention heads are stacked to form a tensor of 4 time steps by 625 pixels out. Although the model is a sequence to sequence model, it is not autoregressive where the following sequence is based on the current sequence generated. All 4 time steps are generated based on the same 52 week of input data.</span></p><h1 class="c14" id="h.o1nmk9m3o9vc"><span class="c9">Results</span></h1><p class="c14"><span class="c1">Models are compared with similar parameters with batch size of 21 samples. Learning rate of 0.001, ADAM optimizer and Mean Absolute Error for the loss function. Both models are trained with 500 epochs or early stop on condition of validation MAE does not make further improvement in the last 50 epochs.</span></p><p class="c14 c33"><span class="c1"></span></p><a id="t.6cf093264ee9c4347043cf3a4719c4c473e2c761"></a><a id="t.1"></a><table class="c36"><tbody><tr class="c48"><td class="c55" colspan="1" rowspan="1"><p class="c5 c33"><span class="c7"></span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c5"><span class="c7">Train MAE</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c5"><span class="c7">Validation MAE</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c5"><span class="c32">Test MAE</span></p></td></tr><tr class="c46"><td class="c55" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Decoder 5 Run Average</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c40"><span class="c7">0.003168</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c40"><span class="c7">0.00501</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c40"><span class="c32">0.008138</span></p></td></tr><tr class="c46"><td class="c55" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Attention Decoder 5 Run Average</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c40"><span class="c7">0.003166</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c40"><span class="c7">0.005014</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c40"><span class="c32">0.00805</span></p></td></tr></tbody></table><p class="c17"><span class="c6">Figure 2: Model Performance Comparison</span></p><p class="c14"><span class="c1">The LSTM Encoder Decoder with Attention matched the performance of a LSTM Encoder Decoder. Within 5 runs, the Attention model slightly surpassed Encoder Decoder model by 1.1 percent. While this is not significant in predictive power, Attention models provide more insight into the data and model themselves. </span></p><p class="c14"><span>After weights are trained, validation set is used to chart the Attention weights vector </span><img src="images/image9.png"><span class="c1">. </span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 321.33px;"><img alt="" src="images/image52.png" style="width: 624.00px; height: 321.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17"><span class="c6">Figure 3a. Attention on LSTM frequency</span></p><p class="c14"><span>Figure 3a above shows Attention weights </span><img src="images/image9.png"><span class="c1">&nbsp;with respect to frequency on the 7 days future prediction. Each individual line represents trained Attention weight for a given single sample of 52 weeks. There is significant Attention on higher frequency for predicting the next 7 days into the future. In other words, the higher frequency updates in the time dimension of 52 weeks are significantly contributing to making near term 7 days predictions. The higher frequencies from the FFT output vector represent correlation values of shorter period time series changes in encoder LSTM out. The function of &nbsp;the encoder is to transform the current time step information to generate the encoder vector. The encoder LSTM time step outputs are directly correlated to the input time step values. Information of the latest value change is captured in this encoded vector. By observing the model looking at short term fluctuations instead of long term seasonality and trends, it can be concluded that predictions are made based on near term training data. This result is consistent with the autocorrelation plot of the validation set provided in Appendix A, where the significant autocorrelation lags of each pixel in KDE map is within 5 to 6 weeks. </span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 580.50px; height: 201.87px;"><img alt="" src="images/image27.png" style="width: 580.50px; height: 201.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 582.50px; height: 204.44px;"><img alt="" src="images/image22.png" style="width: 582.50px; height: 204.44px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17"><span class="c6">Figure 3b. ACF and PACF of the Densest Pixel</span></p><p class="c14"><span class="c1">The Attention weights are charted for 14 days, 21 days, and 28 days into the future in Appendix G. There are no significant changes in the Attention distribution as the prediction period moves into the more distant future. </span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 596.50px; height: 361.65px;"><img alt="" src="images/image59.png" style="width: 596.50px; height: 361.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 378.67px;"><img alt="" src="images/image58.png" style="width: 624.00px; height: 378.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17"><span class="c6">Figure 4: Time vs Frequency Domain with labeled peaks</span></p><p class="c14"><span>Further examining the encoder portion of Attention FFT model revealed specialization of each encoder LSTM cell in capturing a unique set of input information. Above plots in Figure 4 are outputs from encoder LSTM cells that capture approximately singular frequency that can be easily visually identified. Closely observing the rest of the encoder LSTM cell output in time and frequency domain in Appendix F, it can be found that each encoder LSTM cell output specialized in a particular set of model inputs as visualization shows there are variations in time step patterns being outputted by the LSTM cell unit.</span></p><p class="c14"><span class="c6">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE 2015 </span><span class="c6 c10"><a class="c25" href="#h.eensspbhgc0a">[4]</a></span><span class="c6">, </span><span class="c1">revealed performance comparison of the test set results for sentence translation with sentence length from 0 to 60 word count. It was found that the Attention model starts to outperform RNN Encoder Decoder model at translating sentences that contain 20 or more words with training data containing up to 50 words. Since each word is a time step in sequence to sequence sentence translation is a single word, this is equivalent to having autocorrelation between aggregated crime data that are approximately 50 time steps apart. With the tested data format of 7 days as a single time step, the benefit of Attention did not present itself. </span></p><p class="c14"><span>As a post-mortem reflection to the existing comparison of the two models and the tested &nbsp;data format, Attention models may see stronger predictive power compared with Encoder Decoder RNN model, when time step is reduced to a single day from a 7 day window. This shortened time step format would require the model to recall events in a 7 time longer sequence as autocorrelation lags are exacerbated by the shortened time step unit.</span></p><h1 class="c14" id="h.55koqw677ijg"><span class="c9">Conclusions</span></h1><p class="c14"><span>A novel Encoder Decoder and Attention model is tested and compared with the Chicago crime data. The result matched and slightly improved on a layer LSTM Encoder Decoder model. Attention model did not provide significant improvements in predictive power for Chicago crime density in time series. This is due to the lack of long term dependencies in crime occurrences. Crime occurrences typically autocorrelate with the past 5 to 6 weeks of its past history at a given aggregate location. Attention model provided transparency over traditional RNN models.</span></p><h1 class="c14 c26" id="h.ai2xwd7t5npw"><span class="c9"></span></h1><h1 class="c14" id="h.eensspbhgc0a"><span class="c9">References</span></h1><p class="c28"><span class="c35">[1] &quot;Crimes - 2001 to Present | Socrata API Foundry.&quot; 30 Sep. 2011, </span><span class="c10 c35"><a class="c25" href="https://www.google.com/url?q=https://dev.socrata.com/foundry/data.cityofchicago.org/ijzp-q8t2&amp;sa=D&amp;ust=1600125092441000&amp;usg=AOvVaw3iWIPHnRZlWpXaQktL1gdG">https://dev.socrata.com/foundry/data.cityofchicago.org/ijzp-q8t2</a></span><span class="c35 c37">. Accessed 9 Aug. 2020.</span></p><p class="c14"><span class="c35">[2] Zou, Zhimin, et al. &ldquo;Chicago Crime Data.&rdquo; Chicago, </span><span class="c10 c35"><a class="c25" href="https://www.google.com/url?q=https://chicago-crime-data-468.herokuapp.com/&amp;sa=D&amp;ust=1600125092442000&amp;usg=AOvVaw2727ULnXWUmcM8kRfNCEkb">https://chicago-crime-data-468.herokuapp.com/</a></span><span class="c37 c35">. A Dashboard for Visualizing Chicago Crime</span></p><p class="c21"><span class="c37 c35"></span></p><p class="c28"><span class="c35">[3] </span><span class="c20">Vaswani, A., et al. &quot;Attention is all you need. arXiv 2017.&quot; </span><span class="c20 c6">arXiv preprint arXiv:1706.03762</span><span class="c16">&nbsp;(2017).</span></p><p class="c21"><span class="c16"></span></p><p class="c28"><span class="c35">[4] </span><span class="c20">Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. &quot;Neural machine translation by jointly learning to align and translate.&quot; </span><span class="c20 c6">arXiv preprint arXiv:1409.0473</span><span class="c16">&nbsp;(2014).</span></p><p class="c21"><span class="c37 c35"></span></p><p class="c28"><span class="c35">[5] </span><span class="c20">Shih, Shun-Yao, Fan-Keng Sun, and Hung-yi Lee. &quot;Temporal pattern attention for multivariate time series forecasting.&quot; </span><span class="c20 c6">Machine Learning</span><span class="c20">&nbsp;108.8-9 (2019): 1421-1441.</span></p><p class="c14 c33"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><h1 class="c14 c26" id="h.j9zn073pcvhb"><span class="c9"></span></h1><h1 class="c14" id="h.jhwhlixtd484"><span class="c54">Appendi</span><span class="c9">ces</span></h1><ol class="c41 lst-kix_ewqq6femr54-0 start" start="1"><li class="c8"><span class="c1">Autocorrelation and Partial Autocorrelation of training set ordered from the densest pixel</span></li></ol><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image27.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 218.67px;"><img alt="" src="images/image22.png" style="width: 624.00px; height: 218.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image42.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image41.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 216.00px;"><img alt="" src="images/image53.png" style="width: 624.00px; height: 216.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image48.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image57.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image30.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image39.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image45.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29 c33"><span class="c1"></span></p><ol class="c41 lst-kix_ewqq6femr54-0" start="2"><li class="c8"><span>Pixel wise time series of validation set prediction X-axis: timestep unit of 7 days. Y-axis scaled KDE.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 308.00px;"><img alt="" src="images/image25.png" style="width: 624.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 309.33px;"><img alt="" src="images/image55.png" style="width: 624.00px; height: 309.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 310.67px;"><img alt="" src="images/image54.png" style="width: 624.00px; height: 310.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 310.67px;"><img alt="" src="images/image13.png" style="width: 624.00px; height: 310.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 308.00px;"><img alt="" src="images/image50.png" style="width: 624.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c8"><span>Attention LSTM Encoder Decoder. X-axis: timestep unit of 7 days. Y-axis scaled KDE.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 308.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 309.33px;"><img alt="" src="images/image16.png" style="width: 624.00px; height: 309.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 310.67px;"><img alt="" src="images/image47.png" style="width: 624.00px; height: 310.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 310.67px;"><img alt="" src="images/image34.png" style="width: 624.00px; height: 310.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 308.00px;"><img alt="" src="images/image26.png" style="width: 624.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><p class="c29 c33"><span class="c1"></span></p><ol class="c41 lst-kix_ewqq6femr54-0" start="4"><li class="c8"><span class="c1">Detailed Model performance comparison results</span></li></ol><a id="t.966d6a083d463b17ca73c64a340a6e211ab9e71d"></a><a id="t.2"></a><table class="c36"><tbody><tr class="c48"><td class="c42" colspan="1" rowspan="1"><p class="c5 c33"><span class="c7"></span></p></td><td class="c49" colspan="1" rowspan="1"><p class="c5"><span class="c7">Train MAE</span></p></td><td class="c49" colspan="1" rowspan="1"><p class="c5"><span class="c7">Validation MAE</span></p></td><td class="c49" colspan="1" rowspan="1"><p class="c5"><span class="c7">Test MAE</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Decoder Run 1</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00303</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00499</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00788</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Decoder Run 2</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00327</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00508</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00807</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Decoder Run 3</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.0031</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00493</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00863</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Decoder Run 4</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00318</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00502</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00823</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Decoder Run 5</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00326</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00503</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00788</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Attention Decoder Run 1</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00315</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00497</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00828</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Attention Decoder Run 2</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.0031</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00495</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00805</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Attention Decoder Run 3</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00318</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00494</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00795</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Attention Decoder Run 4</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00323</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00513</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00801</span></p></td></tr><tr class="c15"><td class="c42" colspan="1" rowspan="1"><p class="c5"><span class="c7">LSTM Encoder Attention Decoder Run 5</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00317</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00508</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c3">0.00796</span></p></td></tr></tbody></table><p class="c29 c33"><span class="c1"></span></p><ol class="c41 lst-kix_ewqq6femr54-0" start="5"><li class="c8"><span>Crime Visualized in Frequency Domain</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image19.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image28.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image40.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image20.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image46.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image32.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image49.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image35.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.00px; height: 351.00px;"><img alt="" src="images/image17.png" style="width: 337.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c8"><span class="c1">Encoder LSTM cell Input into FFT vs Output from FFT</span></li></ol><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 182.67px;"><img alt="" src="images/image23.png" style="width: 624.00px; height: 182.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 185.33px;"><img alt="" src="images/image44.png" style="width: 624.00px; height: 185.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 180.00px;"><img alt="" src="images/image56.png" style="width: 624.00px; height: 180.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 184.00px;"><img alt="" src="images/image18.png" style="width: 624.00px; height: 184.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 182.67px;"><img alt="" src="images/image36.png" style="width: 624.00px; height: 182.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 185.33px;"><img alt="" src="images/image33.png" style="width: 624.00px; height: 185.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 181.33px;"><img alt="" src="images/image21.png" style="width: 624.00px; height: 181.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 184.00px;"><img alt="" src="images/image43.png" style="width: 624.00px; height: 184.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 182.67px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 182.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 185.33px;"><img alt="" src="images/image31.png" style="width: 624.00px; height: 185.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c41 lst-kix_ewqq6femr54-0" start="7"><li class="c8"><span>Attention versus frequency for validation set</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 321.33px;"><img alt="" src="images/image38.png" style="width: 624.00px; height: 321.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 321.33px;"><img alt="" src="images/image24.png" style="width: 624.00px; height: 321.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 321.33px;"><img alt="" src="images/image29.png" style="width: 624.00px; height: 321.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><p class="c33 c61"><span class="c1"></span></p><p class="c5 c33"><span class="c1"></span></p></body></html>